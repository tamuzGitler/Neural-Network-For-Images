{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heJO4vEqi3U7"
      },
      "source": [
        "## 🪄 Install `wandb` library and login\n",
        "\n",
        "\n",
        "Start by installing the library and logging in to your free account.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WMY2Vy8Xi3U7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6683bf76-21a3-4348-a77e-6f06e56a497f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.20.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.5)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=9b4cc00015aee2ec8ce1072c891b7fc2a5725919c187c68ffad9498167d3f1a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.20.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "t1BpNhjui3U8",
        "outputId": "0bd44035-0f30-4804-a273-72946407522a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Log in to your W&B account\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import pdb\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from keras.datasets import mnist\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.init as init\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "qqXsL_cWnoe-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize AutoEncoder Parameters\n",
        "\n",
        "latent_dim  = 10 # dimensional latent space\n",
        "latent_dimsQ2 = [4,10,40]\n",
        "latent_dimsQ3 = [5,10,20,40]\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "kernel_size = (3,3)\n",
        "weight_decay = 0.01\n",
        "betas = (0.9, 0.999)"
      ],
      "metadata": {
        "id": "3aIcpfT0vu-Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the MNIST dataset\n",
        "\n",
        "\n",
        "# transformer that  converters the data to PyTorch tensors and normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Q3 \n",
        "trainDecorrelation = torch.utils.data.DataLoader(trainset, batch_size=4000, shuffle=True)\n",
        "\n",
        "# Q4\n",
        "trainloaderMLP = torch.utils.data.DataLoader(trainset, batch_size=20, shuffle=True)\n",
        "\n",
        "# Create list with 4 batches in size 20\n",
        "mlpTrainSet = []\n",
        "num_batches = 4\n",
        "\n",
        "for i in range(num_batches):\n",
        "    mlpTrainSet.append(next(iter(trainloaderMLP))) \n"
      ],
      "metadata": {
        "id": "Vv7Z_Ptvn1eB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fba39be-e3d1-4ca8-ab3e-9e2a70c3c13d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 126516455.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 37997394.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31225241.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5822288.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/MNIST_data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define AE\n",
        "\n",
        "# TO use GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# stride=2:\n",
        "# output feature maps will have half the height and width of the input feature maps.\n",
        "# padding='same'\n",
        "# ensures that the output feature maps have the same height and width as the input feature maps.\n",
        "# Using these options together in a convolutional layer in a neural network helps to reduce the size of the feature maps\n",
        "# no spatial structure - autoencoder should only capture the essential features of the image, while disregarding any spatial relationships between pixels.\n",
        "################ original ################\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # init - layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, latent_dim)\n",
        "  \n",
        "        # inits - weights (Xavier initialization)\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "                init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "        # save total parameters of the encoder\n",
        "        self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        # init - layers\n",
        "        self.fc1 = nn.Linear(latent_dim, 32 * 7 * 7)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=16)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        \n",
        "        # inits - weights (Xavier initialization)\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.ConvTranspose2d) or isinstance(layer, nn.Linear):\n",
        "                init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "        # save total parameters of the decoder        \n",
        "        self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = x.view(x.size(0), 32, 7, 7) \n",
        "        x = F.relu(self.conv1(self.bn1(x)))\n",
        "        x = F.sigmoid(self.conv2(self.bn2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "################ II - more layers ################\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, latent_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # init - layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(num_features=16)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(num_features=32)\n",
        "#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(num_features=64)\n",
        "        \n",
        "#         self.fc1 = nn.Linear(64 * 4 * 4, latent_dim)\n",
        "  \n",
        "#         # inits - weights (Xavier initialization)\n",
        "#         for layer in self.modules():\n",
        "#             if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "#                 init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "#         # save total parameters of the encoder\n",
        "#         self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.bn1(self.conv1(x)))\n",
        "#         x = F.relu(self.bn2(self.conv2(x)))\n",
        "#         x = F.relu(self.bn3(self.conv3(x)))\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, latent_dim):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         # init - layers\n",
        "#         self.fc1 = nn.Linear(latent_dim, 64 * 4 * 4)\n",
        "#         self.bn1 = nn.BatchNorm2d(num_features=64)\n",
        "#         self.conv1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(num_features=32)\n",
        "#         self.conv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(num_features=16)\n",
        "#         self.conv3 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "       \n",
        "        \n",
        "#         # inits - weights (Xavier initialization)\n",
        "#         for layer in self.modules():\n",
        "#             if isinstance(layer, nn.ConvTranspose2d) or isinstance(layer, nn.Linear):\n",
        "#                 init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "#         # save total parameters of the decoder        \n",
        "#         self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc1(x)\n",
        "#         x = x.view(x.size(0), 64,4, 4) \n",
        "\n",
        "#         x = F.relu(self.conv1(self.bn1(x)))\n",
        "\n",
        "#         x = F.relu(self.conv2(self.bn2(x)))\n",
        "\n",
        "#         x = F.sigmoid(self.conv3(self.bn3(x)))\n",
        "\n",
        "#         return x\n",
        "\n",
        "############### II - less layers ################\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, latent_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # init - layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(num_features=16)\n",
        " \n",
        "#         self.fc1 = nn.Linear(16 * 14 * 14, latent_dim)\n",
        "  \n",
        "#         # inits - weights (Xavier initialization)\n",
        "#         for layer in self.modules():\n",
        "#             if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "#                 init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "#         # save total parameters of the encoder\n",
        "#         self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.bn1(self.conv1(x)))\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, latent_dim):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         # init - layers\n",
        "#         self.fc1 = nn.Linear(latent_dim, 16 * 14 * 14)\n",
        "#         self.bn1 = nn.BatchNorm2d(num_features=16)\n",
        "#         self.conv1 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        \n",
        "#         # inits - weights (Xavier initialization)\n",
        "#         for layer in self.modules():\n",
        "#             if isinstance(layer, nn.ConvTranspose2d) or isinstance(layer, nn.Linear):\n",
        "#                 init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "#         # save total parameters of the decoder        \n",
        "#         self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc1(x)\n",
        "#         x = x.view(x.size(0), 16, 14, 14) \n",
        "#         x = F.sigmoid(self.conv1(self.bn1(x)))\n",
        "#         return x\n",
        "\n",
        "#op 3\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, latent_dim):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # init - layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "#         self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "#         self.fc1 = nn.Linear(128 * 2 * 2, latent_dim)\n",
        "  \n",
        "#         # inits - weights (Xavier initialization)\n",
        "#         for layer in self.modules():\n",
        "#             if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "#                 init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "#         # save total parameters of the encoder\n",
        "#         self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = F.relu(self.conv1(x))\n",
        "\n",
        "#         x = F.relu(self.conv2(x))\n",
        "\n",
        "#         x = F.relu(self.conv3(x))\n",
        "\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "#         x = self.fc1(x)\n",
        "#         return x\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, latent_dim):\n",
        "#         super().__init__()\n",
        "#         # init - layers\n",
        "#         self.fc1 = nn.Linear(latent_dim, 128 * 2 * 2)\n",
        "#         self.conv1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "#         self.conv2 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
        "#         self.conv3 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "#         self.conv4 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        \n",
        "#         # inits - weights (Xavier initialization)\n",
        "#         for layer in self.modules():\n",
        "#             if isinstance(layer, nn.ConvTranspose2d) or isinstance(layer, nn.Linear):\n",
        "#                 init.xavier_uniform_(layer.weight)\n",
        "        \n",
        "#         # save total parameters of the decoder        \n",
        "#         self.total_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc1(x)\n",
        "#         x = x.view(x.size(0), 128, 2, 2) \n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.sigmoid(self.conv4(x))\n",
        "#         return x\n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        # init Model\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(latent_dim)\n",
        "\n",
        "        # save total parameters of the AutoEncoder \n",
        "        self.total_params = self.encoder.total_params + self.decoder.total_params\n",
        "\n",
        "    def forward(self, x):\n",
        "        encodedX = self.encoder(x)\n",
        "        decodedX = self.decoder(encodedX)\n",
        "        return decodedX\n",
        "\n"
      ],
      "metadata": {
        "id": "4oCr-rxLywSG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Q1 \n",
        "# #@title Question 1 - Auto-Encoding (also good for Q4)\n",
        "autoEncoder = AutoEncoder(latent_dim).to(device)\n",
        "criterion = nn.MSELoss() #measures the mean squared error (squared L2 norm) between each element in the input x and reconstructed decodedX\n",
        "\n",
        "optimizer = optim.AdamW(autoEncoder.parameters(), lr=lr, betas=betas, weight_decay=weight_decay) #AdamW adds l2 regulization to prevent overfitting\n",
        "autoEncoders = [autoEncoder]\n",
        "optimizers = [optimizer]\n",
        "latent_dims = [latent_dim]\n",
        "print(f\"Total number of parameters: {autoEncoder.total_params}\")\n"
      ],
      "metadata": {
        "id": "JwRL55sRgE8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af75c124-2f3e-4a07-e5f5-1968034b220b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 42699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Q2 - Interpolation\n",
        "# create 3 autEncoders with different latent_dim\n",
        "autoEncoders = []\n",
        "optimizers = []\n",
        "criterion = nn.MSELoss() #measures the mean squared error (squared L2 norm) between each element in the input x and reconstructed decodedX\n",
        "latent_dims = latent_dimsQ2\n",
        "\n",
        "for i,latent_dim in enumerate((latent_dimsQ2)):\n",
        "    autoEncoders.append(AutoEncoder(latent_dim).to(device))\n",
        "    optimizers.append(optim.AdamW(autoEncoders[i].parameters(), lr=lr, betas=betas, weight_decay=weight_decay)) #AdamW adds l2 regulization to prevent overfitting\n"
      ],
      "metadata": {
        "id": "Y-k5MC6pgaeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title Q3 - init encoders\n",
        "# # create 3 autEncoders with different latent_dim\n",
        "autoEncoders = []\n",
        "optimizers = []\n",
        "criterion = nn.MSELoss() #measures the mean squared error (squared L2 norm) between each element in the input x and reconstructed decodedX\n",
        "latent_dims = latent_dimsQ3\n",
        "for i,latent_dim in enumerate((latent_dimsQ3)):\n",
        "    autoEncoders.append(AutoEncoder(latent_dim).to(device)) # init AutoEncoder\n",
        "    optimizers.append(optim.AdamW(autoEncoders[i].parameters(), lr=lr, betas=betas, weight_decay=weight_decay)) #AdamW adds l2 regulization to prevent overfitting\n"
      ],
      "metadata": {
        "id": "QgNzLj5GgV3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test AE and calc test loss\n",
        "def evaluate_loss(autoEncoder):\n",
        "  \n",
        "    # evaluate mode\n",
        "    autoEncoder.eval()\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0\n",
        "\n",
        "        for data in testloader:\n",
        "            # get the inputs\n",
        "            images = data[0].to(device)\n",
        "\n",
        "            # reconstruct image\n",
        "            outputs = autoEncoder(images)\n",
        "\n",
        "            # calculate and accumulate loss\n",
        "            loss = criterion(outputs, images)\n",
        "            running_loss += loss.item()\n",
        "  \n",
        "\n",
        "    test_loss = running_loss / len(testloader)\n",
        "    return test_loss \n",
        "\n",
        "# Q4 - MLP,encoder Loss\n",
        "\n",
        "def evaluate_accuracy_mlp(mlp, encoder):\n",
        "    # evaluation mode\n",
        "    encoder.eval()\n",
        "    mlp.eval()\n",
        "\n",
        "    # init variables\n",
        "    correct_pred = 0\n",
        "    total_pred = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels =  data[0].to(device), data[1].to(device)\n",
        "            outputs = mlp(encoder(images))\n",
        "            predictions = torch.max(outputs, 1)[1]\n",
        "            # count the correct predictions\n",
        "            correct_pred += (predictions == labels).sum().item()\n",
        "            total_pred += labels.size(0) #count \n",
        "    \n",
        "    # calculate accuracy\n",
        "    accuracy = 100 * correct_pred / total_pred\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "-nrx0gefwbgX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the network & Loss function\n",
        "\n",
        "for i in range(len(autoEncoders)):\n",
        "    latent_dim = latent_dims[i]\n",
        "    autoEncoder = autoEncoders[i]\n",
        "    optimizer = optimizers[i]\n",
        "\n",
        "    if (len(autoEncoders) == 1): # only 1 autoEncoder - Q1\n",
        "        latent_dim = 10\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # use this wandb init to save autoencoder loss\n",
        "    # # 🐝 1️⃣ Start a new run to track this script\n",
        "    # wandb.init(\n",
        "    #     # Set the project where this run will be logged\n",
        "    #     project=\"ex2-nn\", \n",
        "    #     # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    #     name=f\"epochs: {epochs}, batch size: {batch_size}, lr: {lr}, filter size: {kernel_size}, latent_dim: {latent_dim}\", \n",
        "    #     # Track hyperparameters and run metadata\n",
        "    #     config={\n",
        "    #     \"learning_rate\": lr,\n",
        "    #     \"architecture\": \"AutoEncoder\",\n",
        "    #     \"dataset\": \"MNIST\",\n",
        "    #     \"epochs\": epochs,\n",
        "    #     \"latent_dim\": latent_dim,\n",
        "    #     })\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        print(\"##### Epoch \" + str(epoch)+ \" #####\")               \n",
        "        \n",
        "        # train mode\n",
        "        autoEncoder.train() \n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Train \n",
        "        for data in trainloader:\n",
        "            # get the inputs\n",
        "            images = data[0].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # reconstruct image\n",
        "            outputs = autoEncoder(images)\n",
        "\n",
        "            # calculate loss\n",
        "            loss = criterion(outputs, images)\n",
        "            \n",
        "            # forward + backward + optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # accumulate loss\n",
        "            running_loss += loss.item()\n",
        "          \n",
        "\n",
        "        # calculate and save train and test loss    \n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        test_loss = evaluate_loss(autoEncoder)\n",
        "        # use this wandb init to save autoencoder loss\n",
        "        # wandb.log({\"Train Loss\": train_loss,\"test Loss\": test_loss})\n",
        "\n",
        "    # calculate and print total time of training\n",
        "    end_time = time.time()\n",
        "    total_time = str(end_time - start_time)\n",
        "    print('Finished Training, it took:' + total_time + ' seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvNjMNECoyEa",
        "outputId": "9437ed77-30f6-4787-fed8-9450ed46b281"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Epoch 0 #####\n",
            "##### Epoch 1 #####\n",
            "##### Epoch 2 #####\n",
            "##### Epoch 3 #####\n",
            "##### Epoch 4 #####\n",
            "##### Epoch 5 #####\n",
            "##### Epoch 6 #####\n",
            "##### Epoch 7 #####\n",
            "##### Epoch 8 #####\n",
            "##### Epoch 9 #####\n",
            "##### Epoch 10 #####\n",
            "##### Epoch 11 #####\n",
            "##### Epoch 12 #####\n",
            "##### Epoch 13 #####\n",
            "##### Epoch 14 #####\n",
            "##### Epoch 15 #####\n",
            "##### Epoch 16 #####\n",
            "##### Epoch 17 #####\n",
            "##### Epoch 18 #####\n",
            "##### Epoch 19 #####\n",
            "Finished Training, it took:240.2503125667572 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title Q2 - Interpolation\n",
        "# randomly choose two images from testset\n",
        "idx1 = np.random.randint(low=0, high=len(testset))\n",
        "idx2 = np.random.randint(low=0, high=len(testset))\n",
        "img1, digit1 = testset[idx1]\n",
        "img2, digit2 = testset[idx2]\n",
        "alphas = [0.2, 0.4, 0.6, 0.8] # to interpolate with different alphas\n",
        "\n",
        "# choose again till we get different digits\n",
        "while(digit1 == digit2):\n",
        "    idx2 = np.random.randint(0, len(testset))\n",
        "    img2, digit2 = testset[idx2]\n",
        "\n",
        "# prepare images for interploation\n",
        "img1ForInter = img1.unsqueeze(dim=1).to(device)\n",
        "img2ForInter = img2.unsqueeze(dim=1).to(device) \n",
        "# change dimensions - [1,28,28] -> [28,28] for displaying\n",
        "img1 = img1.squeeze().cpu()\n",
        "img2 = img2.squeeze().cpu()\n",
        "\n",
        "# plot images for every autoEncoder for comperation\n",
        "for i in range(len(autoEncoders)):\n",
        "    print(\"autoEncoder with latent dim: {}\".format(latent_dimsQ2[i]))\n",
        "    autoEncoder = autoEncoders[i]\n",
        "\n",
        "    # interpolate with alphas\n",
        "    interImgs = []\n",
        "    for alpha in alphas:\n",
        "        interImg = (autoEncoder.decoder((alpha * autoEncoder.encoder(img1ForInter)) +\n",
        "                                      (1-alpha) * autoEncoder.encoder(img2ForInter)))\n",
        "        interImg = interImg.squeeze().cpu().detach().numpy()\n",
        "        interImgs.append(interImg)\n",
        "\n",
        " \n",
        "    # Create images & description arrays\n",
        "    images = [img1,img2] + interImgs\n",
        "    description = [digit1,digit2] + alphas\n",
        "    \n",
        "    # Create and fill subplot with images and their labels\n",
        "    fig2, axs2 = plt.subplots(2, 3)\n",
        "    for row in range(2):\n",
        "      for col in range(3):\n",
        "        descrip = \"Alpha\"\n",
        "        if row == 0 and (col == 0 or col == 1):\n",
        "            descrip = \"Digit\"\n",
        "        axs2[row][col].imshow(images[3 * row + col], cmap='gray') # plot img1\n",
        "        axs2[row][col].set_title('{}: {}'.format(descrip,description[3 * row + col]))\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "zqWQYj92ONWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Q3 - Decorrelation\n",
        "\n",
        "# get a batch of 4000 images and labels and transfer to Device\n",
        "images, labels = next(iter(trainDecorrelation)) # not going to use labels\n",
        "images = images.to(device)\n",
        "\n",
        "correlations = []\n",
        "\n",
        "# encode images with 4 different encoders \n",
        "for i in range(len(autoEncoders)):\n",
        "    latent_dim = latent_dimsQ3[i]\n",
        "    print(\"autoEncoder with latent dim: {}\".format(latent_dim))\n",
        "    autoEncoder = autoEncoders[i]\n",
        "    autoEncoder.encoder.eval()\n",
        "\n",
        "    # encode images and calc perason correlation\n",
        "    encodedImages = autoEncoder.encoder(images)\n",
        "    corr_matrix = np.corrcoef(encodedImages.T.cpu().detach().numpy())\n",
        "    #compute the correlation matrix for all pairs of coordinates and then compute an average measure of correlation\n",
        "    correlation = np.mean(np.abs(corr_matrix - np.eye(corr_matrix.shape[0])))  \n",
        "    correlations.append(correlation)\n",
        "\n",
        "# Create a plot with correlationsand latent dimensions\n",
        "plt.scatter(latent_dimsQ3, correlations)\n",
        "# Set  x,y axis labels and title \n",
        "plt.xlabel('Latent Dimension')\n",
        "plt.ylabel('Pearson Correlation')\n",
        "plt.title('Pearson Correlations with respect to the latent space dimension')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5DYFfDsXaSAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Q4 - Transfer Learning\n",
        "    # 🐝 1️⃣ Start a new run to track this script\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"ex2-nn\", \n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=f\"epochs: {epochs}, batch size: {batch_size}, lr: {lr}, filter size: {kernel_size}, latent_dim: {latent_dim}\", \n",
        "    # Track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": lr,\n",
        "    \"architecture\": \"AutoEncoder\",\n",
        "    \"dataset\": \"MNIST\",\n",
        "    \"epochs\": epochs,\n",
        "    \"latent_dim\": latent_dim,\n",
        "    })\n",
        "\n",
        "#Define MLP \n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=10, out_features=32)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)  \n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = F.softmax(self.fc3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Q4.i) train MLP only (i already have the images,labels)\n",
        "def train_model1():\n",
        "    # Init Models, Critetion and Optimizer        \n",
        "    mlp = MLP().to(device)\n",
        "    encoder = autoEncoder.encoder # pre-trained encoder\n",
        "    criterionMLP = nn.CrossEntropyLoss() \n",
        "    optimizerMLP = optim.Adam(mlp.parameters(), lr=lr, betas=betas, weight_decay=weight_decay) #AdamW adds l2 regulization to prevent overfitting    \n",
        "    mlp1_loss = []\n",
        "    mlp1_accuracy = []\n",
        "\n",
        "\n",
        "    # exclude encoder's weights from optimization\n",
        "    for param in encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        mlp.train()\n",
        "        encoder.train()\n",
        "        running_loss = 0\n",
        "        for data in mlpTrainSet:\n",
        "            images, labels = data[0].to(device),data[1].to(device)\n",
        "            # zero the parameter gradients - only MLP params\n",
        "            optimizerMLP.zero_grad()\n",
        "            outputs = encoder(images)\n",
        "            outputs = mlp(outputs)\n",
        "\n",
        "            # calculate loss and accumulate loss\n",
        "            loss = criterionMLP(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "             # update MLP and encoder weights\n",
        "            loss.backward()\n",
        "            optimizerMLP.step()\n",
        "\n",
        "        # calculate and save train and test loss    \n",
        "        mlp1_loss.append(running_loss / len(trainloader))\n",
        "        mlp1_accuracy.append(evaluate_accuracy_mlp(mlp, encoder))\n",
        "    \n",
        "    return mlp1_loss,mlp1_accuracy\n",
        "    \n",
        "\n",
        "\n",
        "# Q4.ii) train MLP + Encoder        \n",
        "# Init Models, Critetion and Optimizer  \n",
        "def train_model2():\n",
        "\n",
        "    mlp = MLP().to(device)\n",
        "    encoder = autoEncoder.encoder # pre-trained encoder\n",
        "    # include encoder's weights in optimization\n",
        "    for param in encoder.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    combined_params = list(mlp.parameters()) + list(encoder.parameters())\n",
        "    criterionMLP = nn.CrossEntropyLoss() \n",
        "    optimizerMLP = optim.Adam(combined_params, lr=lr, betas=betas, weight_decay=weight_decay) #AdamW adds l2 regulization to prevent overfitting    \n",
        "    mlp2_loss = []\n",
        "    mlp2_accuracy = []\n",
        "    print(\"second\")\n",
        "    # Train\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        mlp.train()\n",
        "        encoder.train()\n",
        "        running_loss = 0 \n",
        "        for data in mlpTrainSet:\n",
        "            images, labels = data[0].to(device),data[1].to(device)\n",
        "            # zero the parameter gradients - only MLP params\n",
        "            optimizerMLP.zero_grad()\n",
        "            outputs = encoder(images)\n",
        "            outputs = mlp(outputs)\n",
        "\n",
        "            # calculate loss and accumulate loss\n",
        "            loss = criterionMLP(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "             # update MLP and encoder weights\n",
        "            loss.backward()\n",
        "            optimizerMLP.step()\n",
        "\n",
        "        # calculate and save train and test loss    \n",
        "        mlp2_loss.append(running_loss / len(trainloader))\n",
        "        mlp2_accuracy.append(evaluate_accuracy_mlp(mlp, encoder))\n",
        "\n",
        "\n",
        "    return mlp2_loss,mlp2_accuracy\n",
        "    \n",
        "mlp1_loss,mlp1_accuracy = train_model1()\n",
        "mlp2_loss,mlp2_accuracy = train_model2()\n",
        "for i in range(len(mlp1_loss)):\n",
        "    wandb.log({\"MLP1 Loss\": mlp1_loss[i],\"MLP2 Loss\": mlp2_loss[i],\"MLP1 Accuracy\": mlp1_accuracy[i],\"MLP2 Accuracy\": mlp2_accuracy[i]})\n",
        "\n",
        "\n",
        "\n",
        "print(\"done\")\n",
        "\n",
        "# calculate and save train and test loss    \n",
        "\n"
      ],
      "metadata": {
        "id": "MaqoXl_L-ZqS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d55641c4-6cea-47cf-f550-a78231caef99"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:8djsh65n) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MLP1 Accuracy</td><td>▁▂▃▄▅▅▆▆▇▇▇▇▇███████</td></tr><tr><td>MLP1 Loss</td><td>███▇▇▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>MLP2 Accuracy</td><td>▁▁▄▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>MLP2 Loss</td><td>██▇▇▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MLP1 Accuracy</td><td>62.23</td></tr><tr><td>MLP1 Loss</td><td>0.00787</td></tr><tr><td>MLP2 Accuracy</td><td>60.83</td></tr><tr><td>MLP2 Loss</td><td>0.00747</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">epochs: 20, batch size: 64, lr: 0.001, filter size: (3, 3), latent_dim: 10</strong> at: <a href='https://wandb.ai/nn-for-images/ex2-nn/runs/8djsh65n' target=\"_blank\">https://wandb.ai/nn-for-images/ex2-nn/runs/8djsh65n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230423_100632-8djsh65n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:8djsh65n). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230423_101309-d5a4kvm3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nn-for-images/ex2-nn/runs/d5a4kvm3' target=\"_blank\">epochs: 20, batch size: 64, lr: 0.001, filter size: (3, 3), latent_dim: 10</a></strong> to <a href='https://wandb.ai/nn-for-images/ex2-nn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nn-for-images/ex2-nn' target=\"_blank\">https://wandb.ai/nn-for-images/ex2-nn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nn-for-images/ex2-nn/runs/d5a4kvm3' target=\"_blank\">https://wandb.ai/nn-for-images/ex2-nn/runs/d5a4kvm3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 18.11%\n",
            "Accuracy: 24.92%\n",
            "Accuracy: 31.93%\n",
            "Accuracy: 37.36%\n",
            "Accuracy: 41.55%\n",
            "Accuracy: 44.86%\n",
            "Accuracy: 47.15%\n",
            "Accuracy: 48.71%\n",
            "Accuracy: 49.73%\n",
            "Accuracy: 50.69%\n",
            "Accuracy: 51.74%\n",
            "Accuracy: 52.38%\n",
            "Accuracy: 53.12%\n",
            "Accuracy: 53.91%\n",
            "Accuracy: 54.65%\n",
            "Accuracy: 55.61%\n",
            "Accuracy: 56.49%\n",
            "Accuracy: 57.23%\n",
            "Accuracy: 58.18%\n",
            "Accuracy: 59.09%\n",
            "second\n",
            "Accuracy: 12.09%\n",
            "Accuracy: 11.58%\n",
            "Accuracy: 23.11%\n",
            "Accuracy: 31.14%\n",
            "Accuracy: 37.80%\n",
            "Accuracy: 45.01%\n",
            "Accuracy: 52.48%\n",
            "Accuracy: 56.43%\n",
            "Accuracy: 58.60%\n",
            "Accuracy: 59.88%\n",
            "Accuracy: 60.98%\n",
            "Accuracy: 61.69%\n",
            "Accuracy: 62.37%\n",
            "Accuracy: 63.03%\n",
            "Accuracy: 63.60%\n",
            "Accuracy: 64.55%\n",
            "Accuracy: 65.61%\n",
            "Accuracy: 66.71%\n",
            "Accuracy: 67.67%\n",
            "Accuracy: 68.67%\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "LtBZ50QviXoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}